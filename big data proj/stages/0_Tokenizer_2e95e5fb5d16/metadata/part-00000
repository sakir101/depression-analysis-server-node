{"class":"org.apache.spark.ml.feature.Tokenizer","timestamp":1697873586192,"sparkVersion":"3.1.1","uid":"Tokenizer_2e95e5fb5d16","paramMap":{"outputCol":"words","inputCol":"clean_text"},"defaultParamMap":{"outputCol":"Tokenizer_2e95e5fb5d16__output"}}
